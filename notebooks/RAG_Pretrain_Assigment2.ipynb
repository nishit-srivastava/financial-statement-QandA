{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3f39c3-5d58-4cd1-8cd3-ef4941b4c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install camelot-py pypdf nltk sentence_transformers faiss-cpu rank_bm25 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865b6281-5ef1-4a9d-8c86-581ef77adca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf6414c-f0a3-410d-a6fe-6cd76914e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"TCS_2024-25.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153f8b2d-5b03-44ca-94b2-481485baaefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Segment                                               Data\n",
      "0    financial_results    \\nMarch 31, December 31, March 31, March 31,...\n",
      "1  segment_information  March 31, December 31, March 31, March 31, Mar...\n",
      "2        balance_sheet  As at As at\\nMarch 31, 2025 March 31, 2024\\nAS...\n",
      "3           cash_flows   \\nSelect explanatory notes to the Statement o...\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to collect text\n",
    "annual_result = {\n",
    "    \"financial_results\": \"\",\n",
    "    \"segment_information\": \"\",\n",
    "    \"balance_sheet\": \"\",\n",
    "    \"cash_flows\": \"\"\n",
    "}\n",
    "\n",
    "# Extract and classify text\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if \"consolidated\" in text_lower and \"financial results\" in text_lower:\n",
    "            annual_result[\"financial_results\"] += text + \"\\n\"\n",
    "        \n",
    "        if \"consolidated\" in text_lower and \"segment information\" in text_lower:\n",
    "            annual_result[\"segment_information\"] += text + \"\\n\"\n",
    "        \n",
    "        if \"consolidated\" in text_lower and \"balance sheet\" in text_lower:\n",
    "            annual_result[\"balance_sheet\"] += text + \"\\n\"\n",
    "        \n",
    "        if \"consolidated\" in text_lower and \"cash flows\" in text_lower:\n",
    "            annual_result[\"cash_flows\"] += text + \"\\n\"\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_annual_result = pd.DataFrame({\n",
    "    \"Segment\": list(annual_result.keys()),\n",
    "    \"Data\": list(annual_result.values())\n",
    "})\n",
    "\n",
    "print(df_annual_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf7c1d7-13ee-4a0c-8e77-f92eb7c44134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_result = df_annual_result.assign(\n",
    "    Data=df_annual_result[\"Data\"].str.split(\"\\n\")\n",
    ").explode(\"Data\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226ce33f-0d04-4c53-984e-26d97f1477c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_result[\"Data\"] = (\n",
    "    df_annual_result[\"Data\"]\n",
    "    .str.replace(r\"[.,()]\", \"\", regex=True)  # remove . , ( )\n",
    "    .str.split()                             # split on any whitespace\n",
    "    .str.join(\" \")                           # rejoin with single spaces\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1bf4c3-def6-4907-8736-059fe1fcdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(row):\n",
    "    parts = row[\"Data\"].split()\n",
    "    n_extra = 5  # total col1–col5\n",
    "\n",
    "    if row[\"Segment\"] in [\"financial_results\", \"segment_information\"]:\n",
    "        # Last 5 parts go into col1–col5\n",
    "        last_parts = parts[-5:]\n",
    "        first_part = \" \".join(parts[:-5])\n",
    "        return pd.Series([first_part] + last_parts)\n",
    "\n",
    "    elif row[\"Segment\"] in [\"balance_sheet\", \"cash_flows\"]:\n",
    "        # Last 2 parts go into col4 and col5\n",
    "        last_parts = parts[-2:]\n",
    "        first_part = \" \".join(parts[:-2])\n",
    "        return pd.Series([first_part] + [None, None, None] + last_parts)\n",
    "\n",
    "    else:\n",
    "        return pd.Series([row[\"Data\"]] + [None] * n_extra)\n",
    "\n",
    "\n",
    "\n",
    "# Apply with different column counts\n",
    "df_split = df_annual_result.apply(split_data, axis=1)\n",
    "\n",
    "# Rename columns dynamically for clarity\n",
    "df_split.columns = [\"attribute\", \"col1\", \"col2\", \"col3\", \"2025\", \"2024\"]\n",
    "\n",
    "# Combine with original DataFrame if needed\n",
    "df_result = pd.concat([df_annual_result, df_split], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6132de-50a1-4b29-aed8-8652f88cc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(val):\n",
    "    try:\n",
    "        return float(val).is_integer()\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Keep only rows where both are integers\n",
    "df_result= df_result[df_result[\"2025\"].apply(is_int)]\n",
    "df_result= df_result[df_result[\"2024\"].apply(is_int)]\n",
    "df_result.drop(columns=[\"col1\",\"col2\",\"col3\",\"Data\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166df6c1-fbaa-47cb-9dcb-deadc904fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['text'] = (\n",
    "    \"company's \" + df_result['attribute'] +\n",
    "    \" in 2024 it is \" + df_result[\"2024\"].astype(str) +\n",
    "    \" and for 2025 it is \" + df_result[\"2025\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73457806-e4b5-484e-a0a5-86b42708ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = ' '.join(df_result['text'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727f5e48-044d-4aa1-888b-96307ddf5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/gaurav/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 35 chunks\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int = 0) -> List[Dict]:\n",
    "    \"\"\"Split text into chunks with metadata and unique IDs.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text_str = \" \".join(chunk_tokens)\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"text\": chunk_text_str,\n",
    "            \"metadata\": {\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"start_token\": start,\n",
    "                \"end_token\": end\n",
    "            }\n",
    "        })\n",
    "\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example\n",
    "chunks_100 = chunk_text(combined_text, chunk_size=100)\n",
    "chunks_400 = chunk_text(combined_text, chunk_size=400)\n",
    "\n",
    "all_chunks = chunks_100 + chunks_400\n",
    "print(f\"Created {len(all_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ed3450-e6fb-4a34-9a33-371a0b14b46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1824e4351d7e4254aa7f9077e08a57c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719b100a4eba40b6a89392a59f5dbf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c467e62c767413b9871434356c7cb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1db9483ab6247a5b060c65dbd56603c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b912b61a10c407790c845f921733c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55bca1d684a40158c814fec77e18915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b313cb4710d43e49a60b89f0b18274a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d405e50fa83949de94bfd9e74621d310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac166dd34e14cae93756f854a127867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865863300840477fad167ce387ac82da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07091f8501924af2a721f440f732389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Dense embeddings\n",
    "dense_embeddings = embedding_model.encode([chunk[\"text\"] for chunk in all_chunks], convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "dimension = dense_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(dense_embeddings)\n",
    "\n",
    "# Sparse index (BM25)\n",
    "tokenized_corpus = [word_tokenize(chunk[\"text\"].lower()) for chunk in all_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "573acc55-3b21-49ca-bbfd-c9e00ff7a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, top_n: int = 5, alpha: float = 0.5):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining FAISS (dense) and BM25 (sparse).\n",
    "    alpha: weight for dense retrieval (0 to 1).\n",
    "    \"\"\"\n",
    "    # Preprocess query\n",
    "    query_clean = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", query.lower())\n",
    "    \n",
    "    # Dense retrieval\n",
    "    query_embedding = embedding_model.encode([query_clean], convert_to_numpy=True)\n",
    "    dense_scores, dense_indices = faiss_index.search(query_embedding, top_n)\n",
    "    dense_results = [(idx, 1 / (1 + score)) for idx, score in zip(dense_indices[0], dense_scores[0])]\n",
    "    \n",
    "    # Sparse retrieval\n",
    "    sparse_scores = bm25.get_scores(word_tokenize(query_clean))\n",
    "    sparse_indices = np.argsort(sparse_scores)[::-1][:top_n]\n",
    "    sparse_results = [(idx, sparse_scores[idx]) for idx in sparse_indices]\n",
    "    \n",
    "    # Score fusion\n",
    "    scores_dict = {}\n",
    "    for idx, score in dense_results:\n",
    "        scores_dict[idx] = scores_dict.get(idx, 0) + alpha * score\n",
    "    for idx, score in sparse_results:\n",
    "        scores_dict[idx] = scores_dict.get(idx, 0) + (1 - alpha) * score\n",
    "    \n",
    "    # Sort by combined score\n",
    "    final_results = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return [(all_chunks[idx][\"text\"], score) for idx, score in final_results]\n",
    "\n",
    "# Example search\n",
    "#results = hybrid_search(\"what is total tax expense for year 2025\", top_n=5, alpha=0.6)\n",
    "#for text, score in results:\n",
    "#    print(f\"Score: {score:.4f} | Chunk: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "271a516c-0ed7-48f7-92bd-dbc1047446a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /media/gaurav/ubuntudata/SemThree/models/llama2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  2943.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "......................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   312.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/media/gaurav/ubuntudata/SemThree/models/llama2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,\n",
    "    chat_format=\"llama-2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c47ae05-ea8b-4b69-b7fe-69558c8b3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What were the total expenses for FY 2025?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a47157-15fb-4f9b-ab1f-d462b8d9cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is 778 and for 2025 it is 796 company 's depreciation and amortisation expense in 2024 it is 4985 and for 2025 it is 5242 company 's other expenses in 2024 it is 32764 and for 2025 it is 30481 company 's total expenses in 2024 it is 182360 and for 2025 it is 193955 company 's profit before exceptional item and tax in 2024 it is 62955 and for 2025 it is 65331 company 's profit before tax in 2024 it is 61997 and for 2025 it is 65331 company 's current tax in 2024 it is 15864company 's in 2024 it is 2024 and for 2025 it is 2025 company 's revenue from operations in 2024 it is 240893 and for 2025 it is 255324 company 's other income in 2024 it is 4422 and for 2025 it is 3962 company 's total income in 2024 it is 245315 and for 2025 it is 259286 company 's employee benefit expenses in 2024 it is 140131 and for 2025 it is 145788 company 's cost of equipment and software licences in 2024 it is 3702 and for 2025 it is 11648 company 's finance costs in 2024company 's in 2024 it is 2024 and for 2025 it is 2025 company 's revenue from operations in 2024 it is 240893 and for 2025 it is 255324 company 's other income in 2024 it is 4422 and for 2025 it is 3962 company 's total income in 2024 it is 245315 and for 2025 it is 259286 company 's employee benefit expenses in 2024 it is 140131 and for 2025 it is 145788 company 's cost of equipment and software licences in 2024 it is 3702 and for 2025 it is 11648 company 's finance costs in 2024 it is 778 and for 2025 it is 796 company 's depreciation and amortisation expense in 2024 it is 4985 and for 2025 it is 5242 company 's other expenses in 2024 it is 32764 and for 2025 it is 30481 company 's total expenses in 2024 it is 182360 and for 2025 it is 193955 company 's profit before exceptional item and tax in 2024 it is 62955 and for 2025 it is 65331 company 's profit before tax in 2024 it is 61997 and for 2025 it is 65331 company 's current tax in 2024 it is 15864 and for 2025 it is 16910 company 's deferred tax in 2024 it is 34 and for 2025 it is 376 company 's total tax expense in 2024 it is 15898 and for 2025 it is 16534 company 's profit for the year in 2024 it is 46099 and for 2025 it is 48797 company 's remeasurement of defined employee benefit plans in 2024 it is 2 and for 2025 it is 106 company 's in 2024 it is 6 and for 2025 it is 24 company 's in 2024 it is 11 and for 2025 it is 18 company 's in 2024 it is 237 and for 2025 it is 593 company 's in 2024 it is 1 and for 2025 it is 1 company 's net change in time value of derivatives designated as cash flow hedges in 2024 it is 13 and for 2025 it is 9 company 's in 2024 it is 44 and for 2025 it is 262 company 's income tax on items that will be reclassified subsequently to profit or loss in 2024 it is 39 and for 2025 it is 146 company 's total other comprehensive income / losses in 2024 itit is 8225 company 's consumer business in 2024 it is 10252 and for 2025 it is 11222 company 's communication media and technology in 2024 it is 10918 and for 2025 it is 9582 company 's life sciences and healthcare in 2024 it is 7611 and for 2025 it is 7448 company 's others in 2024 it is 4673 and for 2025 it is 5795 company 's total in 2024 it is 64296 and for 2025 it is 67407 company 's unallocable expenses * in 2024 it is 6721 and for 2025 it is 6038 company 's operating incomeit is 39357 and for 2025 it is 40197 company 's communication media and technology in 2024 it is 39391 and for 2025 it is 45893 company 's life sciences and healthcare in 2024 it is 26745 and for 2025 it is 26456 company 's others in 2024 it is 20981 and for 2025 it is 23011 company 's total in 2024 it is 240893 and for 2025 it is 255324 company 's banking financial services and insurance in 2024 it is 23574 and for 2025 it is 25135 company 's manufacturing in 2024 it is 7268 and for 2025 it is 8225 company 's consumer business in 2024 it is 10252 and for 2025 it is 11222 company 's communication media and technology in 2024 it is 10918 and for 2025 it is 9582 company 's life sciences and healthcare in 2024 it is 7611 and for 2025 it is 7448 company 's others in 2024 it is 4673 and for 2025 it is 5795 company 's total in 2024 it is 64296 and for 2025 it is 67407 company 's unallocable expenses * in 2024 it is 6721 and for 2025 it is 6038 company 's operating income in 2024 it is 57575 and for 2025 it is 61369 company 's other income in 2024 it is 4422 and for 2025 it is 3962 company 's profit before tax in 2024 it is 61997 and for 2025 it is 65331 company 's * includes settlement of legal claim of ` 958 crore in the in 2024 it is 2024 and for 2025 it is 31 company 's march 31 2025 march in 2024 it is 2024 and for 2025 it is 31 company 's property plant and equipment in 2024 it is 9376 and for 2025 it is 10978 company 's capital work-in-progress in 2024 it is 1564 and for 2025 it is 1546 company 's right-of-use assets in 2024 it is 7886 and for 2025 it is 9275 company 's goodwill in 2024 it is 1832 and for 2025 it is 1860 company 's other intangible assets in 2024 it is 510 and for 2025 it is 940 company 's investments in 2024 it is 281 and for 2025 it is 275 company 's billed in 2024 it is 127 and for 2025 it is 91 company 's unbilled in 2024 it is 16 and for 2025\n"
     ]
    }
   ],
   "source": [
    "results = hybrid_search(question, top_n=5, alpha=0.6)\n",
    "context=\"\"\n",
    "for text, score in results:\n",
    "    context+=text.lower()\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf98a86-f5da-4533-9882-da7ce8523f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b6795d2-69d6-42a1-8dd9-354c74a38590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama-cpp-python\n",
    "#pip install --upgrade huggingface_hub\n",
    "\n",
    "#huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir ./models/llama2-7b-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b40030-6065-4eea-ba4f-ad205265c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(llm.tokenize(text.encode(\"utf-8\")))\n",
    "\n",
    "# Build prompt safely within context window\n",
    "def build_messages(context, question, max_ctx=4096, reserve=512):\n",
    "    \"\"\"\n",
    "    max_ctx: model's context size\n",
    "    reserve: keep space for system prompt, question, and answer\n",
    "    \"\"\"\n",
    "    system_msg = (\n",
    "        \"Answer STRICTLY and CONCISELY using ONLY the provided context.Provide only answer  \"\n",
    "        \"If the answer is not present, reply exactly: Not found in context.\"\n",
    "    )\n",
    "    system_tokens = count_tokens(system_msg)\n",
    "    question_tokens = count_tokens(question)\n",
    "\n",
    "    # budget for context\n",
    "    budget = max_ctx - (system_tokens + question_tokens + reserve)\n",
    "    context_tokens = llm.tokenize(context.encode(\"utf-8\"))\n",
    "\n",
    "    if len(context_tokens) > budget:\n",
    "        context_tokens = context_tokens[:budget]  # truncate\n",
    "        context = llm.detokenize(context_tokens).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df84a249-3e41-4085-80ad-f53d5197468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_query(query: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns False if query is irrelevant or harmful.\n",
    "    \"\"\"\n",
    "    harmful_keywords = [\"suicide\", \"kill\", \"drugs\", \"weapon\", \"terrorism\"]\n",
    "    irrelevant_keywords = [\"joke\", \"story\", \"sing\", \"poem\"]  # customize to your use-case\n",
    "\n",
    "    q_lower = query.lower()\n",
    "    if any(k in q_lower for k in harmful_keywords):\n",
    "        print(\"⚠️ Blocked harmful input.\")\n",
    "        return False\n",
    "    if any(k in q_lower for k in irrelevant_keywords):\n",
    "        print(\"⚠️ Irrelevant query, skipping.\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Usage\n",
    "if not validate_query(question):\n",
    "    exit()  # or return a safe response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56bc0148-008a-4a34-ae33-061dd9c7fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_output(response: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensures response is grounded in the provided context.\n",
    "    If response contains info not in context, replace with guardrail message.\n",
    "    \"\"\"\n",
    "    # Simple heuristic: all words in response should appear in context\n",
    "    # (you can replace with embedding similarity check for better accuracy)\n",
    "    context_lower = context.lower()\n",
    "    for word in response.split():\n",
    "        if word.lower()  in context_lower:\n",
    "            return response\n",
    "\n",
    "    return \"Not found in context.\"\n",
    "\n",
    "# Usage\n",
    "#raw_output = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "#safe_output = validate_output(raw_output, context)\n",
    "#print(safe_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e4b5e-d4c7-4ced-8c7f-0d3e5a3e4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inferance(question):\n",
    "    results = hybrid_search(question, top_n=5, alpha=0.6)\n",
    "    context=\"\"\n",
    "    for text, score in results:\n",
    "        context+=text.lower()\n",
    "    messages = build_messages(context, question)\n",
    "    resp = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(model_inferance(\"What was TCS’s consolidated net profit in FY 2025?\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9a2d9ce-7d92-43a8-8fb8-69c1dd74ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 464 prefix-match hit, remaining 1894 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  158107.03 ms /  1894 tokens (   83.48 ms per token,    11.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9683.83 ms /    31 runs   (  312.38 ms per token,     3.20 tokens per second)\n",
      "llama_perf_context_print:       total time =  167808.35 ms /  1925 tokens\n",
      "llama_perf_context_print:    graphs reused =         29\n",
      "Llama.generate: 56 prefix-match hit, remaining 1722 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  133945.74 ms /  1722 tokens (   77.78 ms per token,    12.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8594.12 ms /    28 runs   (  306.93 ms per token,     3.26 tokens per second)\n",
      "llama_perf_context_print:       total time =  142555.69 ms /  1750 tokens\n",
      "llama_perf_context_print:    graphs reused =         26\n",
      "Llama.generate: 1107 prefix-match hit, remaining 1259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  107617.04 ms /  1259 tokens (   85.48 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12057.50 ms /    33 runs   (  365.38 ms per token,     2.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  119694.29 ms /  1292 tokens\n",
      "llama_perf_context_print:    graphs reused =         31\n",
      "Llama.generate: 56 prefix-match hit, remaining 2393 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  201401.21 ms /  2393 tokens (   84.16 ms per token,    11.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13276.65 ms /    31 runs   (  428.28 ms per token,     2.33 tokens per second)\n",
      "llama_perf_context_print:       total time =  214695.74 ms /  2424 tokens\n",
      "llama_perf_context_print:    graphs reused =         29\n",
      "Llama.generate: 279 prefix-match hit, remaining 1465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  116811.79 ms /  1465 tokens (   79.74 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8581.40 ms /    32 runs   (  268.17 ms per token,     3.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  125411.42 ms /  1497 tokens\n",
      "llama_perf_context_print:    graphs reused =         30\n",
      "Llama.generate: 56 prefix-match hit, remaining 2300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  193439.75 ms /  2300 tokens (   84.10 ms per token,    11.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9931.99 ms /    29 runs   (  342.48 ms per token,     2.92 tokens per second)\n",
      "llama_perf_context_print:       total time =  203388.53 ms /  2329 tokens\n",
      "llama_perf_context_print:    graphs reused =         27\n",
      "Llama.generate: 56 prefix-match hit, remaining 2359 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  194028.23 ms /  2359 tokens (   82.25 ms per token,    12.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10513.06 ms /    31 runs   (  339.13 ms per token,     2.95 tokens per second)\n",
      "llama_perf_context_print:       total time =  204561.61 ms /  2390 tokens\n",
      "llama_perf_context_print:    graphs reused =         29\n",
      "Llama.generate: 56 prefix-match hit, remaining 2302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  186582.77 ms /  2302 tokens (   81.05 ms per token,    12.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16789.84 ms /    30 runs   (  559.66 ms per token,     1.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  203391.15 ms /  2332 tokens\n",
      "llama_perf_context_print:    graphs reused =         28\n",
      "Llama.generate: 56 prefix-match hit, remaining 1722 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  138847.66 ms /  1722 tokens (   80.63 ms per token,    12.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10456.07 ms /    29 runs   (  360.55 ms per token,     2.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  149320.30 ms /  1751 tokens\n",
      "llama_perf_context_print:    graphs reused =         27\n",
      "Llama.generate: 1107 prefix-match hit, remaining 1259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  107315.32 ms /  1259 tokens (   85.24 ms per token,    11.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8633.34 ms /    32 runs   (  269.79 ms per token,     3.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  115967.29 ms /  1291 tokens\n",
      "llama_perf_context_print:    graphs reused =         30\n",
      "Llama.generate: 280 prefix-match hit, remaining 2169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  202118.79 ms\n",
      "llama_perf_context_print: prompt eval time =  179018.20 ms /  2169 tokens (   82.53 ms per token,    12.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10154.69 ms /    31 runs   (  327.57 ms per token,     3.05 tokens per second)\n",
      "llama_perf_context_print:       total time =  189190.00 ms /  2200 tokens\n",
      "llama_perf_context_print:    graphs reused =         29\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "test_questions = [\n",
    "    \"What was the total consolidated income for FY 2025?\",\n",
    "    \"What were employee benefit expenses in FY 2025?\",\n",
    "    \"How much did TCS spend on equipment and software licences in FY 2025?\",\n",
    "    \"What were the total expenses for FY 2025?\",\n",
    "    \"What was profit before tax (PBT) in FY 2025?\",\n",
    "    \"How much was the total tax expense in FY 2025?\",\n",
    "    \"How much other income did TCS earn in FY 2024?\",\n",
    "    \"What was the total consolidated income for FY 2024?\",\n",
    "    \"What were employee benefit expenses in FY 2024?\",\n",
    "    \"How much did TCS spend on equipment and software licences in FY 2024?\",\n",
    "    \"What were the total expenses for FY 2024?\"\n",
    "]\n",
    "\n",
    "expected_answers = [\n",
    "   \"259286\",\n",
    "    \"145788\",\n",
    "    \"11648\",\n",
    "    \"193955\",\n",
    "    \"65331\",\n",
    "    \"16534\",\n",
    "    \"237\",\n",
    "    \"245315\",\n",
    "    \"140131\",\n",
    "    \"3702\",\n",
    "    \"182360\"\n",
    "]\n",
    "results = []\n",
    "\n",
    "for q, expected in zip(test_questions, expected_answers):\n",
    "    start_time = time.time()\n",
    "    output_text=model_inferance(q)\n",
    "    end_time = time.time()\n",
    "    inference_time = round((end_time - start_time) , 2)\n",
    "    accuracy = int(expected.lower() in output_text.lower())\n",
    "    \n",
    "    results.append({\n",
    "        \"Question\": q,\n",
    "        \"Expected\": expected,\n",
    "        \"Model Output\": output_text,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Inference Time (ms)\": inference_time\n",
    "    })\n",
    "    #print(results)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9eaca0e-8d17-4bcd-a292-8805e41dd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('outputRAG.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba7034-ec3b-4b82-91d1-d775ff93aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
